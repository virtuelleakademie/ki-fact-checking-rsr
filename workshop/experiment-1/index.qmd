---
title: "Teil 1: Das Sprachmodell liegt falsch"
subtitle: "Gemeinsames Experiment"
---

::: {.panel-tabset}

## Das Experiment

<section>
:::{.timer #EXP-1 seconds=300 starton=interaction}
:::
</section>

::: {.experiment}

1. Öffne dein Sprachmodell (ChatGPT, Copilot, Claude, oder Gemini)
2. Kopiere den Prompt unten und füge ihn ein
3. Sende die Anfrage ab
4. Notiere dir die Zahl, die das Modell nennt

:::

::: {.prompt}
Wie hoch ist der Anteil der Studierenden an Schweizer Universitäten, die ihr Bachelorstudium in der Regelstudienzeit abschliessen?
:::

## Antworten teilen

::: {.group}

- Teile deine Antwort mit der Gruppe
- Vergleicht die Zahlen: Sind sie gleich oder unterschiedlich?

:::

## Auswertung

::: {.demonstration}

Wir prüfen jetzt gemeinsam die offizielle Quelle:

1. Neuen Browser-Tab öffnen
2. Suchen: `BFS Studiendauer Bachelor Schweiz`
3. Zur BFS-Seite (bfs.admin.ch) navigieren
4. Offizielle Statistik mit den Modell-Antworten vergleichen

:::

## Beobachtung

::: {.reflect}

- Stimmen die Zahlen der Sprachmodelle mit der offiziellen Quelle überein?
- Hat das Modell eine Quelle genannt?
- Wie sicher klang die Antwort?

:::

::: {.key-point}

Antworten von Sprachmodellen können sehr spezifisch und selbstbewusst klingen und trotzdem falsch sein.

**Überzeugend heisst nicht korrekt.**

:::

::: {.warning}
### Die Spezifitätsfalle

Je spezifischer eine Antwort klingt (genaue Zahlen, Daten, Namen), desto vertrauenswürdiger wirkt sie. Aber Spezifität ist kein Beweis für Richtigkeit. Sprachmodelle generieren oft sehr präzise klingende Details, die frei erfunden sind.

:::

:::

:::
